# SolidFire Log Analysis Tool ğŸ”¥

A comprehensive, high-performance log analysis toolkit designed specifically for SolidFire storage system logs. Built with Nushell for powerful data manipulation and analysis, with multi-language preprocessors for maximum performance.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Nushell](https://img.shields.io/badge/Nushell-0.90%2B-blue)](https://nushell.sh)
[![Rust](https://img.shields.io/badge/Rust-1.70%2B-orange)](https://rustlang.org)
[![Python](https://img.shields.io/badge/Python-3.8%2B-green)](https://python.org)

## ğŸ¯ Purpose & Context

SolidFire storage systems generate complex, structured log files containing critical information about cluster health, node status, service operations, and fault conditions. These logs use a unique nested format with:

- **Structured Objects**: `clusterFault={{id=123, type=warning, details=[...]}}`
- **Key-Value Pairs**: `serviceID=1234, status=active, nodeID=5`
- **Nested Arrays**: `details=[drive1, drive2, drive3]`
- **Mixed Data Types**: Strings, integers, floats, booleans, timestamps

Traditional log analysis tools struggle with this format, requiring manual parsing or complex regex operations. This toolkit provides purpose-built parsers that understand SolidFire's log structure, automatically extracting and converting data into analyzable tabular formats.

## ğŸš€ Key Features

### Multi-Engine Architecture
- **5 specialized parsers** optimized for different scenarios
- **Performance range**: 8,000 to 300,000 lines/second
- **Memory management**: Constant memory usage for large files
- **Format support**: CSV, JSON, JSONL, Parquet output

### Advanced Parsing Capabilities
- **Nested object extraction** with automatic flattening
- **Type-aware conversion** (strings â†’ numbers, booleans)
- **Timestamp normalization** with date/time separation
- **Duplicate column handling** with smart naming
- **Malformed entry tolerance** with graceful degradation

### Analysis-Ready Output
- **Consistent column naming** across all parsers
- **Structured data access** via object.field notation
- **Time-series ready** with extracted timestamps
- **Filter-friendly** format for complex queries

## ğŸ“Š Performance Comparison

| Parser | Speed (lines/sec) | Memory Usage | Best For |
|--------|------------------|--------------|----------|
| parsesfv2.nu | ~10,000 | Moderate | General analysis, <1GB files |
| parsesfv3.nu | ~15,000 | Moderate | Optimized analysis, <1GB files |
| parsesfv4.nu | ~8,000 | Constant | Large files, streaming |
| parsesfv5.nu + Rust | ~100,000 | Low | Repeated analysis, >1GB files |
| extract-sf-faults | ~50,000 | Low | Fault analysis only |
| Python preprocessor | ~300,000 | Variable | Massive files, >5GB |

## ğŸ›  Installation & Requirements

### Core Requirements
```bash
# Install Nushell (latest version recommended)
# macOS
brew install nushell

# Linux
curl -L https://github.com/nushell/nushell/releases/latest/download/nu-0.xx.x-x86_64-unknown-linux-gnu.tar.gz | tar xz

# Windows
winget install nushell
```

### Optional Dependencies
```bash
# For preprocessing functions
brew install ripgrep  # or apt-get install ripgrep

# For Rust preprocessor
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
cd sf-parser && cargo build --release

# For Python preprocessor
pip install uv  # Modern Python package manager
uv add polars  # High-performance DataFrame library
```

### Quick Setup
```bash
git clone <repository-url>
cd SF-AnalysisTool-nu

# Test basic functionality
nu -c "use src/parsesfv2.nu; parse-sf-logs 'data/simplelog' --columns-info"

# Build Rust preprocessor (optional)
cd sf-parser && cargo build --release && cd ..

# Install Python dependencies (optional)
uv sync
```

## ğŸ“‹ Use Cases & Scenarios

### 1. ğŸš¨ Incident Response & Troubleshooting

**Scenario**: Storage cluster experiencing performance issues, need to identify root cause from logs.

```bash
# Quick fault analysis
nu -c "
use src/parsesfv4.nu
extract-sf-faults 'data/sf-master.error' 
| where timestamp > '2024-01-01T00:00:00'
| group-by fault_type 
| each { |group| {type: $group.name, count: ($group.items | length)} }
| sort-by count -r
"

# Detailed cluster fault investigation
nu -c "
use src/parsesfv5.nu
let data = parse-sf-logs-rust 'data/sf-master.error'
$data | analyze-cluster-faults 
| where clusterFault_severity in ['critical', 'warning']
| select timestamp clusterFault_type clusterFault_code clusterFault_id
| sort-by timestamp
"
```

**Benefits**: 
- Rapid fault identification
- Time-correlated analysis
- Severity-based filtering
- Trend identification

### 2. ğŸ“ˆ Performance Analysis & Capacity Planning

**Scenario**: Analyzing storage performance trends and predicting capacity needs.

```bash
# Service performance analysis
nu -c "
use src/parsesfv3.nu
parse-sf-logs-fast 'data/sf-master.info.18' 
| where serviceID != null
| group-by serviceID
| each { |group| {
    service: $group.name,
    entries: ($group.items | length),
    nodes: ($group.items | get nodeID | uniq | length),
    last_seen: ($group.items | get timestamp | max)
  }}
| sort-by entries -r
"

# Node health trending
nu -c "
use src/parsesfv5.nu
let data = parse-sf-logs-rust 'data/sf-master.info'
$data | analyze-node-health
| group-by nodeID
| each { |group| {
    node: $group.name,
    status_changes: ($group.items | get status | uniq | length),
    current_status: ($group.items | last | get status),
    master_changes: ($group.items | get masterID | uniq | length)
  }}
"
```

**Benefits**:
- Performance trend identification
- Capacity utilization tracking
- Node health monitoring
- Service distribution analysis

### 3. ğŸ” Compliance & Audit Reporting

**Scenario**: Generate compliance reports showing system activity and change tracking.

```bash
# Configuration change audit
nu -c "
use src/parsesfv2.nu
parse-sf-logs 'data/sf-master.info' 
| where col_1 == 'CONFIG'
| select timestamp col_2 col_3 col_4
| rename timestamp event_time component action details
| sort-by event_time
| to csv
" | save audit_report.csv

# Service status compliance
nu -c "
use src/load_jsonl.nu
load-sf-data --type '*status*' --limit 10000
| where status != null
| group-by date
| each { |group| {
    date: $group.name,
    services_active: ($group.items | where status == 'active' | length),
    services_total: ($group.items | length),
    compliance_pct: (($group.items | where status == 'active' | length) / ($group.items | length) * 100)
  }}
| sort-by date
"
```

**Benefits**:
- Automated compliance reporting
- Change tracking and attribution
- Service level agreement monitoring
- Historical trend documentation

### 4. ğŸ§ª Development & Testing

**Scenario**: Analyzing logs during development and testing phases.

```bash
# Quick log exploration
nu -c "
use src/parsesfv2.nu
parse-sf-logs 'data/simplelog' --columns-info
"

# Sample data extraction for testing
nu -c "
use src/load_jsonl.nu
load-sf-sample 100 
| where clusterFault_type != null
| select timestamp clusterFault_type clusterFault_severity
| to json
" | save test_data.json

# Performance benchmarking
nu -c "
use src/parsesfv3.nu
let start = (date now)
let data = parse-sf-logs-fast 'data/sf-master.info.18'
let end = (date now)
let duration = ($end - $start)
let lines = ($data | length)
print $'Processed ($lines) lines in ($duration)'
print $'Rate: (($lines / ($duration | into int) * 1000000000) | math round) lines/second'
"
```

**Benefits**:
- Rapid prototyping and testing
- Performance validation
- Sample data generation
- Development workflow integration

### 5. ğŸ¢ Enterprise Monitoring & Alerting

**Scenario**: Integrate with monitoring systems for proactive alerting.

```bash
# Critical alert detection
nu -c "
use src/parsesfv4.nu
extract-sf-faults 'data/sf-master.error'
| where fault_type == 'critical' and timestamp > ((date now) - 1hr)
| each { |fault| {
    alert_level: 'CRITICAL',
    timestamp: $fault.timestamp,
    message: $fault.raw_fault,
    action_required: true
  }}
| to json
" | curl -X POST -H 'Content-Type: application/json' -d @- http://monitoring-system/alerts

# Health dashboard data
nu -c "
use src/parsesfv5.nu
let data = parse-sf-logs-rust 'data/sf-master.info'
{
  cluster_health: ($data | analyze-cluster-faults | where clusterFault_severity == 'critical' | length),
  node_count: ($data | analyze-node-health | get nodeID | uniq | length),
  active_services: ($data | analyze-services | where status == 'active' | length),
  last_update: (date now | date format '%Y-%m-%d %H:%M:%S')
}
| to json
"
```

**Benefits**:
- Real-time alerting capability
- Dashboard integration
- Automated monitoring
- Proactive issue detection

## ğŸ“ Project Structure

```
SF-AnalysisTool-nu/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ parsesfv2.nu             # Core parser (comprehensive)
â”‚   â”œâ”€â”€ parsesfv3.nu             # Fast parser (optimized)
â”‚   â”œâ”€â”€ parsesfv4.nu             # Streaming parser (large files)
â”‚   â”œâ”€â”€ parsesfv5.nu             # Rust-accelerated parser
â”‚   â”œâ”€â”€ load_jsonl.nu            # JSONL loader utilities
â”‚   â”œâ”€â”€ preprocess.nu            # Ripgrep preprocessor
â”‚   â”œâ”€â”€ sf_parser.rs             # Rust preprocessor
â”‚   â”œâ”€â”€ sf_preprocessor.py       # Python preprocessor
â”‚   â””â”€â”€ oldcode/                 # Legacy implementations
â”œâ”€â”€ data/                        # Sample log files
â”‚   â”œâ”€â”€ simplelog               # Small test file
â”‚   â”œâ”€â”€ sf-master.info.18       # Realistic test file
â”‚   â”œâ”€â”€ sf-master.error         # Error log sample
â”‚   â””â”€â”€ ...                     # Additional log files
â”œâ”€â”€ output/                      # Processed output files
â”‚   â”œâ”€â”€ *.jsonl                 # JSONL outputs
â”‚   â””â”€â”€ *.parquet               # Parquet outputs
â”œâ”€â”€ sf-parser/                   # Rust preprocessor project
â”‚   â”œâ”€â”€ src/main.rs             # Rust source
â”‚   â”œâ”€â”€ Cargo.toml              # Rust dependencies
â”‚   â””â”€â”€ target/release/         # Compiled binaries
â”œâ”€â”€ docs/                        # Documentation
â”‚   â””â”€â”€ www.nushell.sh/         # Nushell language reference
â”œâ”€â”€ CLAUDE.md                    # Project instructions
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ sf-analysis-tool.1          # Man page
â””â”€â”€ pyproject.toml              # Python dependencies
```

## ğŸ› Parser Selection Guide

### Choose parsesfv2.nu when:
- âœ… File size < 1GB
- âœ… Need full feature support
- âœ… Interactive analysis
- âœ… Complete data extraction required
- âœ… First-time users

### Choose parsesfv3.nu when:
- âœ… File size < 1GB
- âœ… Performance is priority
- âœ… Frequent parsing operations
- âœ… Compatible output format needed
- âœ… CPU-optimized environment

### Choose parsesfv4.nu when:
- âœ… File size > 1GB
- âœ… Limited memory available
- âœ… Only need cluster faults
- âœ… Can process in chunks
- âœ… Long-running operations

### Choose parsesfv5.nu when:
- âœ… File size > 1GB
- âœ… Repeated analysis needed
- âœ… Maximum performance required
- âœ… Have Rust toolchain
- âœ… Batch processing workflows

### Choose Python preprocessor when:
- âœ… File size > 5GB
- âœ… Python environment preferred
- âœ… Parquet output desired
- âœ… Maximum preprocessing speed
- âœ… Integration with Python analytics

## ğŸ“– Quick Start Examples

### Basic Analysis
```bash
# Parse and explore a log file
nu -c "
use src/parsesfv2.nu
let data = parse-sf-logs 'data/sf-master.info.18' --columns-info
print 'Sample data:'
$data | first 5
"
```

### Performance Analysis
```bash
# Compare parser performance
nu -c "
let parsers = [
  {name: 'v2', file: 'src/parsesfv2.nu', func: 'parse-sf-logs'},
  {name: 'v3', file: 'src/parsesfv3.nu', func: 'parse-sf-logs-fast'}
]

$parsers | each { |parser|
  let start = (date now)
  nu -c $'use ($parser.file); ($parser.func) \"data/simplelog\"' | ignore
  let end = (date now)
  {parser: $parser.name, duration: ($end - $start)}
}
"
```

### Data Pipeline
```bash
# Complete analysis pipeline
nu -c "
# 1. Parse logs
use src/parsesfv3.nu
let raw_data = parse-sf-logs-fast 'data/sf-master.info.18'

# 2. Extract cluster faults
let faults = $raw_data | where clusterFault_type != null

# 3. Analyze by severity
let analysis = $faults 
| group-by clusterFault_severity 
| each { |group| {
    severity: $group.name,
    count: ($group.items | length),
    types: ($group.items | get clusterFault_type | uniq)
  }}

# 4. Export results
$analysis | to csv | save fault_analysis.csv
print 'Analysis complete! Results saved to fault_analysis.csv'

# 5. Display summary
$analysis | table
"
```

## ğŸ”§ Advanced Configuration

### Environment Variables
```bash
# Set default chunk size for streaming parser
export SF_CHUNK_SIZE=5000

# Set output directory
export SF_OUTPUT_DIR="./processed_logs"

# Enable debug logging
export SF_DEBUG=1
```

### Custom Analysis Functions
```nushell
# Create custom analysis function
def analyze-service-health [] {
  where serviceID != null
  | group-by serviceID
  | each { |group|
      let items = $group.items
      {
        service_id: $group.name,
        status_count: ($items | length),
        unique_nodes: ($items | get nodeID | uniq | length),
        last_activity: ($items | get timestamp | max),
        health_score: (if ($items | any { |item| $item.status == "error" }) { 0 } else { 100 })
      }
    }
  | sort-by health_score
}

# Use custom function
use src/parsesfv3.nu
parse-sf-logs-fast 'data/sf-master.info.18' | analyze-service-health
```

## ğŸ§ª Testing & Validation

### Run Test Suite
```bash
# Basic functionality test
nu -c "
use src/parsesfv2.nu
let result = parse-sf-logs 'data/simplelog'
assert ($result | length) > 0
print 'Basic parsing test: PASSED'
"

# Performance benchmark
nu scripts/benchmark.nu

# Data integrity test
nu scripts/validate_output.nu
```

### Sample Data Generation
```bash
# Generate test data
nu -c "
use src/load_jsonl.nu
load-sf-sample 1000 
| where clusterFault_type != null
| to json | save test_cluster_faults.json

print 'Test data generated: test_cluster_faults.json'
"
```

## ğŸš€ Performance Tuning

### Memory Optimization
- Use streaming parsers for files > 1GB
- Set appropriate chunk sizes
- Clear intermediate variables
- Use `--limit` parameters when exploring

### CPU Optimization
- Use parsesfv3.nu for CPU-optimized parsing
- Enable Rust preprocessor for repeated analysis
- Utilize parallel processing for multiple files
- Consider Python preprocessor for maximum speed

### I/O Optimization
- Use SSD storage for log files
- Process files locally when possible
- Use JSONL format for intermediate storage
- Consider Parquet for analytical workloads

## ğŸ“Š Monitoring & Observability

### Progress Tracking
```bash
# Monitor parsing progress
tail -f /tmp/sf_parsing.log &
nu -c "
use src/parsesfv4.nu
parse-sf-logs-streaming 'data/large-file.log' 1000 2>/tmp/sf_parsing.log
"
```

### Resource Usage Monitoring
```bash
# Monitor memory usage during parsing
nu -c "
let start_mem = (ps | where name =~ 'nu' | get mem | math sum)
use src/parsesfv2.nu
parse-sf-logs 'data/sf-master.info.18' | ignore
let end_mem = (ps | where name =~ 'nu' | get mem | math sum)
print $'Memory usage: (($end_mem - $start_mem) / 1024 / 1024) MB'
"
```

## ğŸ¤ Contributing

### Development Setup
```bash
# Clone repository
git clone <repository-url>
cd SF-AnalysisTool-nu

# Install development dependencies
uv sync --dev

# Run tests
nu tests/run_all_tests.nu

# Format code (if applicable)
cargo fmt --all  # For Rust code
black src/      # For Python code
```

### Code Standards
- Use `snake_case` for Nushell variables and functions
- Add comprehensive documentation headers
- Test with `data/simplelog` before committing
- Update README.md for new features
- Follow existing error handling patterns

### Adding New Parsers
1. Create new parser file: `src/parsesfv{N}.nu`
2. Add comprehensive documentation header
3. Implement core parsing function
4. Add performance benchmarks
5. Update README.md and man page
6. Add test cases

## ğŸ“ Version History

### v5.0 (Current)
- âœ¨ Rust-accelerated hybrid parser
- âœ¨ Python preprocessor with Polars
- âœ¨ Comprehensive documentation
- âœ¨ Man page and advanced examples
- ğŸ› Improved error handling
- âš¡ Performance optimizations

### v4.0
- âœ¨ Streaming parser for large files
- âœ¨ Ultra-fast fault extractor
- âœ¨ Memory-efficient processing
- ğŸ› Fixed duplicate column handling

### v3.0
- âœ¨ Optimized fast parser
- âœ¨ Improved type conversion
- âš¡ 15-20% performance improvement

### v2.0
- âœ¨ Core parser with full features
- âœ¨ Comprehensive log parsing
- âœ¨ Type-aware conversion
- âœ¨ Timestamp extraction

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support & Troubleshooting

### Common Issues

**Issue**: `parse-sf-logs: command not found`
```bash
# Solution: Import the module first
use src/parsesfv2.nu
parse-sf-logs "your-file.log"
```

**Issue**: Out of memory errors
```bash
# Solution: Use streaming parser
use src/parsesfv4.nu
parse-sf-logs-streaming "large-file.log" 1000
```

**Issue**: Slow parsing performance
```bash
# Solution: Use optimized parser or Rust preprocessor
use src/parsesfv3.nu  # For moderate speedup
# OR
use src/parsesfv5.nu  # For maximum speed
parse-sf-logs-rust "file.log" --preprocess
```

### Getting Help
- ğŸ“š Check the man page: `man ./sf-analysis-tool.1`
- ğŸ’¬ Review examples in this README
- ğŸ› Check existing issues in the repository
- ğŸ’¡ Create new issues for bugs or feature requests

### Performance Troubleshooting
- Monitor memory usage with system tools
- Use appropriate parser for file size
- Consider preprocessing for repeated analysis
- Optimize chunk sizes for streaming operations

---

**Built with â¤ï¸ for the SolidFire community**

*This tool is designed to make SolidFire log analysis accessible, efficient, and insightful. Whether you're troubleshooting issues, monitoring performance, or conducting compliance audits, we've got you covered.*